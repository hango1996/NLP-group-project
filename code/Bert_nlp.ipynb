{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Load required package"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing, metrics\n",
    "from sklearn.metrics.classification import precision_score\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data and the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df  = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer():\n",
    "\n",
    "    def __init__(self, X_data, y_data): \n",
    "        #use pre-trained tokenizers\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) \n",
    "        self.max_length = 512\n",
    "        self.batch_size = 6\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = model_selection.train_test_split(X_data, y_data, test_size = 0.2, random_state = 1)\n",
    "        self.train_encoded = None\n",
    "        self.test_encoded  = None \n",
    "        self.model = None\n",
    "\n",
    "    def convert_example_to_feature(self, review):\n",
    "        # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n",
    "        return self.tokenizer.encode_plus(review, \n",
    "                                 add_special_tokens = True, # add [CLS], [SEP]\n",
    "                                 max_length = self.max_length, # max length of the text that can go to BERT\n",
    "                                 pad_to_max_length = True, # add [PAD] tokens\n",
    "                                 return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "\n",
    "    def map_example_to_dict(self, input_ids, attention_masks, token_type_ids, label):\n",
    "        # map to the expected input to TFBertForSequenceClassification\n",
    "        return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        } , label\n",
    "\n",
    "    def encode_examples(self, X_train, y_train):\n",
    "        # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "        input_ids_list = []\n",
    "        token_type_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        label_list = []\n",
    "    \n",
    "        for i in X_train.index: \n",
    "            bert_input = self.convert_example_to_feature(X_train[i])\n",
    "            input_ids_list.append(bert_input['input_ids'])\n",
    "            token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "            attention_mask_list.append(bert_input['attention_mask'])\n",
    "            label_list.append([y_train[i]])\n",
    "            \n",
    "    \n",
    "        return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(self.map_example_to_dict)\n",
    "\n",
    "    def encoder(self):\n",
    "\n",
    "        self.train_encoded = self.encode_examples(self.X_train, self.y_train).batch(self.batch_size) \n",
    "        self.test_encoded  = self.encode_examples(self.X_test,  self.y_test ).batch(self.batch_size) \n",
    "\n",
    "    #use already prepared TensorFlow models from transformers models\n",
    "    def decoder(self): \n",
    "\n",
    "        ## model initialization \n",
    "\n",
    "        # recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "        learning_rate = 2e-2\n",
    "\n",
    "        # we will do just 1 epoch for illustration, though multiple epochs might be better as                           long as we will not overfit the model\n",
    "        number_of_epochs = 1\n",
    "\n",
    "\n",
    "        # model initialization\n",
    "        self.model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # choosing Adam optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "        # we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "\n",
    "        ## fine tuning \n",
    "        bert_history = self.model.fit(self.train_encoded, epochs=number_of_epochs, validation_data=self.test_encoded)\n",
    "        print(\"a\")\n",
    "        return bert_history\n",
    "        "
   ]
  },
  {
   "source": [
    "## set up Encoder and decoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "Tran = transformer(train_df[\"text\"], train_df[\"target\"])\n",
    "Tran.encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_113', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "14/14 [==============================] - 497s 35s/step - loss: 3.1374 - accuracy: 0.6329 - val_loss: 1.4485 - val_accuracy: 0.7000\n",
      "a\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8cd3e2be50>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "Tran.decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tran.model"
   ]
  }
 ]
}